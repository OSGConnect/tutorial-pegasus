#!/usr/bin/env python3

import os
import argparse
import getpass
import logging
import json
import socket
import re
import sys
from pathlib import Path

from Pegasus.api import (
    Properties,
    Workflow,
    Transformation,
    TransformationCatalog,
    Site,
    SiteCatalog,
    FileServer,
    Operation,
    ReplicaCatalog,
    Container,
    File,
    Directory,
    Arch,
    OS,
    Job,
    PegasusClientError,
    Namespace
)


class VariantCallingWorkflow:
    
    BASE_PATH = Path(__file__).parent.resolve()
    WORK_DIR = Path.home() / "workflows"


    def __init__(self):
        """."""
        self.runs_dir = self.WORK_DIR / "runs"
        self.output_dir = self.WORK_DIR / "outputs"
        self.recipe = None

        self.props = Properties()

        self.wf = Workflow("variant-calling")
        self.tc = TransformationCatalog()
        self.sc = SiteCatalog()
        self.rc = ReplicaCatalog()

        self.wf.add_transformation_catalog(self.tc)
        self.wf.add_site_catalog(self.sc)
        self.wf.add_replica_catalog(self.rc)


    def generate_props(self):
        # set the concurrency limit for the download jobs
        self.props["dagman.fasterq-dump.maxjobs"] = "20"
        # send extra details to the Pegasus team
        self.props["pegasus.catalog.workflow.amqp.url"] = "amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows"
        # non-shared fs, because we want to use OSDF for staging
        self.props["pegasus.data.configuration"] = "nonsharedfs"
        # nicer looking submit dirs
        self.props["pegasus.dir.useTimestamp"] = "true"
        self.props.write()


    def generate_tc(self):

        container_image = "https://download.pegasus.isi.edu/containers/variant-calling/variant-calling-v1.sif"

        fasterq_dump = Transformation(
            'fasterq-dump',
            site='local',
            pfn=self.BASE_PATH / 'bin/fasterq_dump_wrapper',
            is_stageable=True
        ).add_pegasus_profiles(memory="3000", diskspace="10000") \
         .add_profiles(Namespace.DAGMAN, key='category', value='fasterq-dump') \
         .add_profiles(Namespace.CONDOR, key='container_image', value=container_image)
        
        bwa = Transformation(
            'bwa',
            site='condorpool', # in container
            pfn="/opt/bwa/bwa",
            is_stageable=False
        ).add_pegasus_profiles(memory="3000", diskspace="10000") \
         .add_profiles(Namespace.CONDOR, key='container_image', value=container_image)
        
        samtools = Transformation(
            "samtools",
            site="local", 
            pfn=self.BASE_PATH / "bin/samtools_wrapper",
            is_stageable=True
        ).add_pegasus_profiles(memory="3000", diskspace="10000") \
         .add_profiles(Namespace.CONDOR, key='container_image', value=container_image)
        
        bcftools = Transformation(
            "bcftools",
            site="condorpool", # in container
            pfn="/opt/bcftools/bin/bcftools",
            is_stageable=False
        ).add_pegasus_profiles(memory="3000", diskspace="10000") \
         .add_profiles(Namespace.CONDOR, key='container_image', value=container_image)
        
        vcfutils = Transformation(
            "vcfutils",
            site="condorpool", # in container
            pfn="/opt/bcftools/bin/vcfutils.pl",
            is_stageable=False
        ).add_pegasus_profiles(memory="3000", diskspace="10000") \
         .add_profiles(Namespace.CONDOR, key='container_image', value=container_image)

        self.tc.add_transformations(fasterq_dump, bwa, samtools, bcftools, vcfutils)


    def generate_sc(self):

        username = getpass.getuser()
        hostname = socket.gethostname()
        hostname = re.sub("\..*", "", hostname)

        # ap40 has a different local hostname
        if hostname == "ospool-ap2140":
            hostname = "ap40"

        osdf_local_base = f"/ospool/{hostname}/data/{username}"
        if not os.path.exists(osdf_local_base):
            print(f"Unable do determine local OSDF location. Tried {osdf_local_base}")
            sys.exit(1)

        condorpool = (
            Site("condorpool")
            .add_pegasus_profile(style="condor")
        )

        osdf = (
            Site("osdf")
             .add_directories(
                Directory(
                    Directory.SHARED_SCRATCH, f"{osdf_local_base}/staging"
                ).add_file_servers(
                    FileServer(f"osdf://{osdf_local_base}/staging", Operation.ALL)
                )
            )
        ) 

        self.sc.add_sites(condorpool, osdf)


    def generate_workflow(self):

        # set up the reference genome and what files need to be generated by the index job
        ref_path = Path(self.recipe["REF"]).resolve()
        self.rc.add_replica('local', ref_path.name, str(ref_path))

        ref = File(f"{ref_path.name}")
        ref_idx_amb = File(f"{ref_path.name}.amb")
        ref_idx_ann = File(f"{ref_path.name}.ann")
        ref_idx_bwt = File(f"{ref_path.name}.bwt")
        ref_idx_pac = File(f"{ref_path.name}.pac")
        ref_idx_sa = File(f"{ref_path.name}.sa")

        # index the reference file
        index_job = (
            Job("bwa", node_label="ref_genome_index")
            .add_args("index", ref)
            .add_inputs(ref)
            .add_outputs(
                ref_idx_amb,
                ref_idx_ann,
                ref_idx_bwt,
                ref_idx_pac,
                ref_idx_sa,
                stage_out=False
            )
        )
        self.wf.add_jobs(index_job)

        # add jobs for each input
        for sra_id in self.recipe["SRA_IDS"]:

            fastq_1 = File(f"{sra_id}_1.fastq")
            fastq_2 = File(f"{sra_id}_2.fastq")
            sam = File(f"{sra_id}.aligned.sam")
            bam = File(f"{sra_id}.aligned.bam")
            sorted_bam = File(f"{sra_id}.aligned.sorted.bam")
            raw_bcf = File(f"{sra_id}_raw.bcf")
            variants = File(f"{sra_id}_variants.bcf")
            final_variants = File(f"{sra_id}_final_variants.bcf")
            
            # download data from NCBI
            download_job = (
                Job("fasterq-dump", node_label=f"download-{sra_id}")
                .add_args('--split-files', sra_id)
                .add_outputs(fastq_1, fastq_2, stage_out=False)
            )

            # align reads tp reference genome job
            align_job = (
                Job("bwa", node_label="align_reads")
                .add_args("mem", "-t 1", ref, fastq_1, fastq_2)
                .add_inputs(
                    ref,
                    ref_idx_amb,
                    ref_idx_ann,
                    ref_idx_bwt,
                    ref_idx_pac,
                    ref_idx_sa,
                    fastq_1,
                    fastq_2
                )
                .set_stdout(sam, stage_out=False)
            )

            # samtools_wrapper for doing alignment to genome
            bam_job = (
                Job("samtools", node_label="sam_2_bam_converter")
                .add_args(sra_id)
                .add_inputs(sam)
                .add_outputs(bam, sorted_bam, stage_out=False)
            )

            # Variant calling
            # calculating the read coverage of positions in the genome
            coverage_job = (
                Job("bcftools", node_label="calculate_read_coverage")
                .add_args("mpileup", "-O", "b", "-o", raw_bcf, "-f", ref, sorted_bam)
                .add_inputs(ref, sorted_bam)
                .add_outputs(raw_bcf, stage_out=False)
            )

            # detect the single nucleotide variants (SNVs)
            snv_job = (
                Job("bcftools", node_label="detect_snv")
                .add_args("call", "--ploidy", "1", "-m", "-v", "-Ob", "-o", variants, raw_bcf)
                .add_inputs(raw_bcf)
                .add_outputs(variants, stage_out=True)
            )

            # filter and report the SNV variants in variant calling format (VCF)
            vcf_job = (
                Job("bcftools", node_label="variant_calling")
                .add_args("view", "-i", "'QUAL>=20'", "-o", final_variants, variants)
                .add_inputs(variants)
                .add_outputs(final_variants, stage_out=True)
            )
    
            self.wf.add_jobs(
                download_job,
                align_job,
                bam_job,
                coverage_job,
                snv_job,
                vcf_job
            )


    def plan_workflow(self):
        try:
            self.wf.plan(
                dir=str(self.runs_dir),
                output_dir=str(self.output_dir),
                sites=["condorpool"],
                staging_sites={"condorpool": "osdf"},
            ).graph(include_files=False, output="worflow-nofiles.png") \
             .graph(include_files=True, output="workflow-files.png")
        except PegasusClientError as e:
            print(e.output)


    def __call__(self):
        """."""
        parser = argparse.ArgumentParser(description="generate a pegasus workflow")
        parser.add_argument(
            "--recipe",
            dest="recipe",
            default=None,
            required=True,
            help="Recipe",
        )
        args = parser.parse_args(sys.argv[1:])

        self.recipe = Path(args.recipe)

        if not self.recipe.exists():
            raise ValueError("--recipe file either does not exist or is not readable")

        if not self.recipe.is_file():
            raise ValueError("--recipe must be a file")

        self.recipe = json.load(open(self.recipe))

        self.generate_props()
        self.generate_tc()
        self.generate_sc()
        self.generate_workflow()
        self.plan_workflow()


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    wf = VariantCallingWorkflow()
    wf()

